# General
APP_ENV=dev

# API
API_HOST=0.0.0.0
API_PORT=8000

# UI
UI_HOST=0.0.0.0
UI_PORT=7860

# Memory storage (fallback when VECTOR_STORE=jsonl)
MEMORY_PATH=/app/data/memory.jsonl

# Ollama (optional in first cut)
OLLAMA_HOST=ollama
OLLAMA_PORT=11434

########################################
# Embeddings
########################################
# Default zh/EN friendly; optional BGE-M3 for multilingual high quality
EMBED_MODEL=BAAI/bge-small-zh-v1.5
# Add BGE-style query instruction prefixes
EMBED_ADD_QUERY_PREFIX=true
EMBED_QUERY_PREFIX_ZH=为这个句子生成表示以用于检索相关文章：
EMBED_QUERY_PREFIX_EN=Represent this sentence for retrieving related passages:
# ASR model (future): faster-whisper small/base
ASR_MODEL=faster-whisper-small
# Vector store: jsonl (PoC) | chroma | weaviate
########################################
# Vector Store
########################################
# Options: jsonl | chroma (recommended)
VECTOR_STORE=chroma
# Chroma persistence directory (inside container)
CHROMA_PERSIST_DIR=/app/data/chroma
CHROMA_TELEMETRY=false
# Device: cpu | cuda (if available)
DEVICE=cpu
########################################
# Retrieval tuning
########################################
# Default Top-K for queries
TOP_K=5
# Enable lexical BM25 alongside dense retrieval (planned incremental)
ENABLE_BM25=true
# Enable MMR diversification over candidates
USE_MMR=true
MMR_CANDIDATES=20
MMR_LAMBDA=0.5
# Optional cross-encoder reranker (requires transformers + torch)
ENABLE_RERANKER=false
RERANKER_MODEL=BAAI/bge-reranker-base

########################################
# ASR (optional; faster-whisper)
########################################
ASR_ENABLED=true
ASR_MODEL_SIZE=small
ASR_DEVICE=auto
ASR_VAD_FILTER=true

# Retrieval tuning
# Enable MMR re-ranking to improve diversity of results
USE_MMR=true
# Number of initial candidates to consider for MMR selection
MMR_CANDIDATES=20
# MMR trade-off parameter (0..1); higher favors relevance, lower favors diversity
MMR_LAMBDA=0.5
